
单机8卡

以MultiHead_Attention为例的
1. Tensor parallism
  1). Row Parallel (已完成)
  2). Col Parallel(已完成)
2. Data parallism(已完成)
3. Checkpoint(已完成)
4. Mix percision(已完成)

batch_size, head, seq_length, dim = 64, 8, 128, 4096
 Atten (No TP) 11.06398868560791; Atten (with TP) 3.2577245235443115; 

 Atten (No TP) 11.527137756347656; Atten (with TP) 2.727466106414795; 

 Atten (No TP) 11.232461214065552; Atten (with TP) 3.0435214042663574; 

 Atten (No TP) 11.82038140296936; Atten (with TP) 2.6357405185699463; 

 Atten (load checkpoint) 0.01711273193359375; 

 Atten (load checkpoint) 0.01429295539855957; 

 Atten (load checkpoint) 0.014110803604125977; 

 Atten (load checkpoint) 0.007226705551147461; 

 Atten (Mix percision) 0.35045576095581055; 

 Atten (Mix percision) 0.5172052383422852; 

 Atten (Mix percision) 0.7580904960632324; 


 Atten (Mix percision) 0.5356500148773193; 

 Atten (Data parallelism) 0.19030284881591797;
 
 Atten (Data parallelism) 0.19769954681396484; 

 Atten (Data parallelism) 0.1979372501373291; 

 Atten (Data parallelism) 0.19976353645324707; 